{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import xgboost as xgb  \n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON input\n",
    "with open('input.json', 'r') as f:\n",
    "    input_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = input_data['design_state_data']['session_info']['dataset']\n",
    "if os.path.exists(dataset_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "else:\n",
    "    raise FileExistsError(\"The Specified CSV not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numericals(strategy, value):\n",
    "    if strategy == \"Average of values\":\n",
    "        strategy = \"mean\"\n",
    "        imputer = SimpleImputer(strategy=strategy )\n",
    "    else:\n",
    "        strategy = \"constant\"\n",
    "        imputer = SimpleImputer(strategy=strategy ,  fill_value=value)\n",
    "    return imputer\n",
    "\n",
    "def scale_numericals(rescaling):\n",
    "    if rescaling == \"No rescaling\":\n",
    "        return None\n",
    "    else:\n",
    "        return StandardScaler()\n",
    "\n",
    "def hash_text(columns):\n",
    "    if columns == 0:\n",
    "        columns = 1\n",
    "    return  HashingVectorizer(n_features=columns, alternate_sign=False, norm=None)\n",
    "def iterate_features_and_handle(all_features_to_handle, X):\n",
    "    numeric_features = []\n",
    "    numeric_transformers = []\n",
    "    categorical_features = []\n",
    "    categorical_transformers = []\n",
    "    for feature, details in all_features_to_handle.items():\n",
    "        #Purposfully avoiding to process The TargetVariable not sure, why it was added to the Handle Features Json\n",
    "        if details['is_selected'] :\n",
    "            #classify it as categorical or Numerical\n",
    "            if details['feature_variable_type'] == 'numerical':\n",
    "                numeric_features.append(feature)\n",
    "                imputer = impute_numericals(strategy=details['feature_details']['impute_with'], value = details['feature_details']['impute_value'])\n",
    "                scaler = scale_numericals(rescaling = details['feature_details']['rescaling'])\n",
    "                transformers = [('imputer', imputer)]\n",
    "                numeric_transformers.append((feature, Pipeline(transformers)))\n",
    "            elif details['feature_variable_type'] == 'text':\n",
    "                categorical_features.append(feature)\n",
    "                text_vectorizer = hash_text(details['feature_details']['hash_columns'])\n",
    "                categorical_transformers.append((feature, text_vectorizer, feature))\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('numeric', Pipeline(numeric_transformers), numeric_features),\n",
    "        ('categorical', ColumnTransformer(transformers=categorical_transformers), categorical_features)])    \n",
    "    columns = X.columns\n",
    "\n",
    "    return  pd.DataFrame(preprocessor.fit_transform(X), columns=columns)\n",
    "\n",
    "\n",
    "def generate_features(dataset, feature_generation):\n",
    "    # Linear interactions\n",
    "    linear_interactions = feature_generation.get(\"linear_interactions\", [])\n",
    "    for interaction in linear_interactions:\n",
    "        dataset[f\"{interaction[0]}_{interaction[1]}\"] = dataset[interaction[0]] * dataset[interaction[1]]\n",
    "\n",
    "    # Polynomial interactions\n",
    "    polynomial_interactions = feature_generation.get(\"polynomial_interactions\", [])\n",
    "    poly = PolynomialFeatures(include_bias=False)\n",
    "    for interaction in polynomial_interactions:\n",
    "        interaction_split = interaction.split(\"/\")\n",
    "        transformed = poly.fit_transform(dataset[[interaction_split[0], interaction_split[1]]])\n",
    "        for i in range(transformed.shape[1]):\n",
    "            dataset[f\"poly_{interaction_split[0]}_{interaction_split[1]}_{i}\"] = transformed[:, i]\n",
    "\n",
    "    # Explicit pairwise interactions\n",
    "    explicit_pairwise_interactions = feature_generation.get(\"explicit_pairwise_interactions\", [])\n",
    "    for interaction in explicit_pairwise_interactions:\n",
    "        interaction_split = interaction.split(\"/\")\n",
    "        dataset[f\"{interaction_split[0]}_{interaction_split[1]}\"] = dataset[interaction_split[0]] * dataset[interaction_split[1]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def reduce_features(dataset, config, target_variable):\n",
    "    if config[\"feature_reduction_method\"] == \"Tree-based\":\n",
    "        num_of_features_to_keep = int(config[\"num_of_features_to_keep\"])\n",
    "        num_of_trees = int(config[\"num_of_trees\"])\n",
    "        depth_of_trees = int(config[\"depth_of_trees\"])\n",
    "        \n",
    "        # Select features and target variable\n",
    "        X = dataset.drop(columns=[target_variable])\n",
    "        y = dataset[target_variable]\n",
    "        \n",
    "        # Initialize Random Forest Regressor\n",
    "        rf = RandomForestRegressor(n_estimators=num_of_trees, max_depth=depth_of_trees, random_state=42)\n",
    "        \n",
    "        # Fit Random Forest model\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "        \n",
    "        # Select top k features\n",
    "        top_features = feature_importances.nlargest(num_of_features_to_keep).index.tolist()\n",
    "        \n",
    "        # Update dataset with selected features\n",
    "        dataset = dataset[top_features + [target_variable]]\n",
    "        \n",
    "        return dataset\n",
    "    else:\n",
    "        print(\"Unsupported feature reduction method. Please choose 'Tree-based'.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "def partition_data(data, config):\n",
    "    \"\"\"\n",
    "    Partition data based on the configuration specified in the JSON.\n",
    "    \n",
    "    Args:\n",
    "    - data (DataFrame): The dataset to be partitioned.\n",
    "    - config (dict): JSON configuration specifying the partitioning details.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the partitioned data.\n",
    "    \"\"\"\n",
    "    # Extract configuration parameters\n",
    "    policy = config.get(\"policy\", \"Split the dataset\")\n",
    "    time_variable = config.get(\"time_variable\", None)\n",
    "    sampling_method = config.get(\"sampling_method\", \"No sampling(whole data)\")\n",
    "    split = config.get(\"split\", \"Randomly\")\n",
    "    k_fold = config.get(\"k_fold\", False)\n",
    "    train_ratio = config.get(\"train_ratio\", 0)\n",
    "    random_seed = config.get(\"random_seed\", None)\n",
    "    \n",
    "    # Partition data based on policy\n",
    "    if policy == \"Split the dataset\":\n",
    "        if sampling_method == \"No sampling(whole data)\":\n",
    "            if split == \"Randomly\":\n",
    "                if not k_fold:\n",
    "                    if train_ratio > 0:\n",
    "                        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=random_seed)\n",
    "                    else:\n",
    "                        raise ValueError(\"train_ratio must be greater than 0.\")\n",
    "                else:\n",
    "                    pass  \n",
    "            else:\n",
    "                raise ValueError(\"Only random split is implemented currently.\")\n",
    "        else:\n",
    "            raise ValueError(\"Sampling method other than 'No sampling(whole data)' is not implemented.\")\n",
    "    else:\n",
    "        raise ValueError(\"Policy other than 'Split the dataset' is not implemented.\")\n",
    "    \n",
    "    if k_fold:\n",
    "        kf = KFold(n_splits=k_fold, shuffle=True, random_state=random_seed)\n",
    "        fold_data = {}\n",
    "        fold_index = 1\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            fold_train_data, fold_test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            fold_data[f\"fold_{fold_index}\"] = {\"train\": fold_train_data, \"test\": fold_test_data}\n",
    "            fold_index += 1\n",
    "        return fold_data\n",
    "    else:\n",
    "        return {\"train\": pd.DataFrame(train_data), \"test\": pd.DataFrame(test_data)}\n",
    "\n",
    "\n",
    "\n",
    "def create_models_from_json(data):\n",
    "    models = {}\n",
    "    for key, value in data.items():\n",
    "        if value[\"is_selected\"] == True:\n",
    "            if key == \"RandomForestClassifier\":\n",
    "                model = RandomForestClassifier(n_estimators=value[\"max_trees\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf_min_value\"],\n",
    "                                                max_depth=value[\"max_depth\"])\n",
    "            elif key == \"RandomForestRegressor\":\n",
    "                model = RandomForestRegressor(n_estimators=value[\"max_trees\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf_min_value\"],\n",
    "                                                max_depth=value[\"max_depth\"])\n",
    "            elif key == \"GBTClassifier\":\n",
    "                model = GradientBoostingClassifier(n_estimators=value[\"fixed_number\"],\n",
    "                                                    max_depth=value[\"max_depth\"])\n",
    "            elif key == \"GBTRegressor\":\n",
    "                model = GradientBoostingRegressor(n_estimators=value[\"fixed_number\"],\n",
    "                                                    max_depth=value[\"max_depth\"])\n",
    "            elif key == \"LinearRegression\":\n",
    "                model = LinearRegression()\n",
    "            elif key == \"LogisticRegression\":\n",
    "                model = LogisticRegression()\n",
    "            elif key == \"RidgeRegression\":\n",
    "                model = Ridge()\n",
    "            elif key == \"LassoRegression\":\n",
    "                model = Lasso()\n",
    "            elif key == \"ElasticNetRegression\":\n",
    "                model = ElasticNet()\n",
    "            elif key == \"DecisionTreeRegressor\":\n",
    "                model = DecisionTreeRegressor(max_depth=value[\"max_depth\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"DecisionTreeClassifier\":\n",
    "                model = DecisionTreeClassifier(max_depth=value[\"max_depth\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"SVM\":\n",
    "                model = SVC()\n",
    "            elif key == \"SVR\":\n",
    "                model = SVR()\n",
    "            elif key == \"SGD\":\n",
    "                model = SGDRegressor(alpha=value[\"alpha_value\"][0])\n",
    "            elif key == \"KNN\":\n",
    "                model = KNeighborsClassifier(n_neighbors=value[\"k_value\"][0])\n",
    "            elif key == \"KNNRegressor\":\n",
    "                model = KNeighborsRegressor(n_neighbors=value[\"k_value\"][0])\n",
    "            elif key == \"extra_random_trees\":\n",
    "                model = ExtraTreesRegressor(n_estimators=value[\"num_of_trees\"][1],\n",
    "                                            max_depth=value[\"max_depth\"][1],\n",
    "                                            min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"neural_network\":\n",
    "                model = MLPClassifier(hidden_layer_sizes=value[\"hidden_layer_sizes\"])\n",
    "            elif key == \"neural_network_regressor\":\n",
    "                model = MLPRegressor(hidden_layer_sizes=value[\"hidden_layer_sizes\"])\n",
    "            elif key == \"xg_boost\":\n",
    "                model = xgb.XGBRegressor(n_estimators=value[\"max_num_of_trees\"],\n",
    "                                            max_depth=value[\"max_depth_of_tree\"][1],\n",
    "                                            learning_rate=value[\"learningRate\"][0],\n",
    "                                            reg_alpha=value[\"l1_regularization\"][0],\n",
    "                                            reg_lambda=value[\"l2_regularization\"][0],\n",
    "                                            gamma=value[\"gamma\"][0],\n",
    "                                            min_child_weight=value[\"min_child_weight\"][0],\n",
    "                                            subsample=value[\"sub_sample\"][0],\n",
    "                                            colsample_bytree=value[\"col_sample_by_tree\"][0],\n",
    "                                            early_stopping_rounds=value[\"early_stopping_rounds\"])\n",
    "\n",
    "            models[key] = model\n",
    "\n",
    "            return models\n",
    "\n",
    "# Now you have a dictionary containing all the created models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models_with_metrics(models, X_train, y_train, X_test, y_test, metrics):\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_prob = None\n",
    "\n",
    "        roc_auc = None\n",
    "        f1_optimal_threshold = None\n",
    "        lift_at_threshold = None\n",
    "\n",
    "        if y_prob is not None:\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        if metrics[\"optomize_model_hyperparameters_for\"] == \"AUC\" and roc_auc is not None:\n",
    "            model_metrics = roc_auc\n",
    "        elif metrics[\"optimize_threshold_for\"] == \"F1 Score\":\n",
    "            f1_optimal_threshold = 0\n",
    "            max_f1 = 0\n",
    "            for threshold in np.linspace(0.1, 0.9, 9):\n",
    "                if y_prob:\n",
    "                    y_pred_threshold = (y_prob >= threshold).astype(int)\n",
    "                    f1 = f1_score(y_test, y_pred_threshold)\n",
    "                    if f1 > max_f1:\n",
    "                        max_f1 = f1\n",
    "                        f1_optimal_threshold = threshold\n",
    "            model_metrics = max_f1\n",
    "        else:\n",
    "            model_metrics = None\n",
    "\n",
    "        if metrics[\"compute_lift_at\"] > 0 and y_prob is not None:\n",
    "            conf_matrix = confusion_matrix(y_test, y_prob >= f1_optimal_threshold)\n",
    "            true_positives = conf_matrix[1, 1]\n",
    "            lift_at_threshold = true_positives / (y_test.sum() / len(y_test))\n",
    "\n",
    "        results[name] = {\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"f1_optimal_threshold\": f1_optimal_threshold,\n",
    "            \"lift_at_threshold\": lift_at_threshold\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def apply_weighting_strategy(dataset, config):\n",
    "    if config[\"weighting_strategy_method\"] == \"Sample weights\":\n",
    "        weight_variable = config[\"weighting_strategy_weight_variable\"]\n",
    "        if weight_variable in dataset.columns:\n",
    "            sample_weights = dataset[weight_variable]\n",
    "            return sample_weights\n",
    "        else:\n",
    "            print(f\"Weight variable '{weight_variable}' not found in dataset. Weighting strategy not applied.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Unsupported weighting strategy method. Please choose 'Sample weights'.\")\n",
    "        return None\n",
    "\n",
    "def apply_probability_calibration(X_train, y_train, X_test, config):\n",
    "    if config[\"probability_calibration_method\"] == \"Sigmoid - Platt Scaling\":\n",
    "        calibrated_clf = CalibratedClassifierCV(method='sigmoid', cv='prefit')\n",
    "        calibrated_clf.fit(X_train, y_train)\n",
    "        calibrated_proba = calibrated_clf.predict_proba(X_test)\n",
    "        return calibrated_proba\n",
    "    else:\n",
    "        print(\"Unsupported probability calibration method. Please choose 'Sigmoid - Platt Scaling'.\")\n",
    "        return None\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def optimize_models(models, X_train, y_train, hyperparameters):\n",
    "    optimized_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hyperparameters[\"stratergy\"] == \"Grid Search\":\n",
    "            param_grid = {}  # Define hyperparameters grid here\n",
    "            if name == \"RandomForestClassifier\":\n",
    "                param_grid = {\n",
    "                    \"n_estimators\": [10, 20, 30],\n",
    "                    \"max_depth\": [None, 10, 20]\n",
    "                    # Add other hyperparameters specific to RandomForestClassifier\n",
    "                }\n",
    "            elif name == \"RandomForestRegressor\":\n",
    "                param_grid = {\n",
    "                    \"n_estimators\": [10, 20, 30],\n",
    "                    \"max_depth\": [None, 10, 20]\n",
    "                    # Add other hyperparameters specific to RandomForestRegressor\n",
    "                }\n",
    "            # Add similar elif blocks for other models\n",
    "\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=hyperparameters[\"num_of_folds\"],\n",
    "                                       scoring='roc_auc', n_jobs=hyperparameters[\"parallelism\"])\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            optimized_model = grid_search.best_estimator_\n",
    "            optimized_models[name] = optimized_model\n",
    "\n",
    "    return optimized_models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pwayk\\anaconda3\\envs\\jtm\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RandomForestRegressor': {'roc_auc': None,\n",
       "  'f1_optimal_threshold': 0,\n",
       "  'lift_at_threshold': None}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_path = input_data['design_state_data']['session_info']['dataset']\n",
    "if os.path.exists(dataset_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "else:\n",
    "    raise FileExistsError(\"The Specified CSV not found\")\n",
    "\n",
    "all_features_to_handle = input_data['design_state_data']['feature_handling']\n",
    "target_column = input_data['design_state_data']['target']['target']\n",
    "features_to_generate = input_data['design_state_data']['feature_generation']\n",
    "feature_reduction_json  = input_data['design_state_data']['feature_reduction']\n",
    "model_config = input_data['design_state_data'][\"algorithms\"]\n",
    "training_config  = input_data['design_state_data']['train']\n",
    "metrics = input_data['design_state_data'][\"metrics\"]\n",
    "hyperparameters = input_data['design_state_data'][\"hyperparameters\"]\n",
    "\n",
    "X = data\n",
    "y = data[target_column]\n",
    "\n",
    "X = iterate_features_and_handle(all_features_to_handle, X)\n",
    "\n",
    "X = generate_features(X,features_to_generate )\n",
    "\n",
    "X = reduce_features(X, feature_reduction_json, target_column)\n",
    "\n",
    "X = partition_data(X, training_config)\n",
    "\n",
    "x_train = X['train'].drop(columns=[target_column])\n",
    "y_train = X['train'][target_column]\n",
    "x_test = X['test'].drop(columns=[target_column])\n",
    "y_test = X['train'][target_column]\n",
    "\n",
    "models = create_models_from_json(model_config)\n",
    "optimized_models = optimize_models(models, x_train, y_train, hyperparameters)\n",
    "\n",
    "results = evaluate_models_with_metrics(optimized_models, x_train, y_train, x_test, y_test, metrics)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
