{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import xgboost as xgb  \n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON input\n",
    "with open('input.json', 'r') as f:\n",
    "    input_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numericals(strategy, value):\n",
    "    if strategy == \"Average of values\":\n",
    "        strategy = \"mean\"\n",
    "        imputer = SimpleImputer(strategy=strategy )\n",
    "    else:\n",
    "        strategy = \"constant\"\n",
    "        imputer = SimpleImputer(strategy=strategy ,  fill_value=value)\n",
    "    return imputer\n",
    "\n",
    "def scale_numericals(rescaling):\n",
    "    if rescaling == \"No rescaling\":\n",
    "        return None\n",
    "    else:\n",
    "        return StandardScaler()\n",
    "\n",
    "def hash_text(columns):\n",
    "    if columns == 0:\n",
    "        columns = 1\n",
    "    return  HashingVectorizer(n_features=columns, alternate_sign=False, norm=None)\n",
    "def iterate_features_and_handle(all_features_to_handle, X):\n",
    "    numeric_features = []\n",
    "    numeric_transformers = []\n",
    "    categorical_features = []\n",
    "    categorical_transformers = []\n",
    "    for feature, details in all_features_to_handle.items():\n",
    "        #Purposfully avoiding to process The TargetVariable not sure, why it was added to the Handle Features Json\n",
    "        if details['is_selected'] :\n",
    "            #classify it as categorical or Numerical\n",
    "            if details['feature_variable_type'] == 'numerical':\n",
    "                numeric_features.append(feature)\n",
    "                imputer = impute_numericals(strategy=details['feature_details']['impute_with'], value = details['feature_details']['impute_value'])\n",
    "                scaler = scale_numericals(rescaling = details['feature_details']['rescaling'])\n",
    "                transformers = [('imputer', imputer)]\n",
    "                numeric_transformers.append((feature, Pipeline(transformers)))\n",
    "            elif details['feature_variable_type'] == 'text':\n",
    "                categorical_features.append(feature)\n",
    "                text_vectorizer = hash_text(details['feature_details']['hash_columns'])\n",
    "                categorical_transformers.append((feature, text_vectorizer, feature))\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('numeric', Pipeline(numeric_transformers), numeric_features),\n",
    "        ('categorical', ColumnTransformer(transformers=categorical_transformers), categorical_features)])    \n",
    "    columns = X.columns\n",
    "\n",
    "    return  pd.DataFrame(preprocessor.fit_transform(X), columns=columns)\n",
    "\n",
    "\n",
    "def generate_features(dataset, feature_generation):\n",
    "    # Linear interactions\n",
    "    linear_interactions = feature_generation.get(\"linear_interactions\", [])\n",
    "    for interaction in linear_interactions:\n",
    "        dataset[f\"{interaction[0]}_{interaction[1]}\"] = dataset[interaction[0]] * dataset[interaction[1]]\n",
    "\n",
    "    # Polynomial interactions\n",
    "    polynomial_interactions = feature_generation.get(\"polynomial_interactions\", [])\n",
    "    poly = PolynomialFeatures(include_bias=False)\n",
    "    for interaction in polynomial_interactions:\n",
    "        interaction_split = interaction.split(\"/\")\n",
    "        transformed = poly.fit_transform(dataset[[interaction_split[0], interaction_split[1]]])\n",
    "        for i in range(transformed.shape[1]):\n",
    "            dataset[f\"poly_{interaction_split[0]}_{interaction_split[1]}_{i}\"] = transformed[:, i]\n",
    "\n",
    "    # Explicit pairwise interactions\n",
    "    explicit_pairwise_interactions = feature_generation.get(\"explicit_pairwise_interactions\", [])\n",
    "    for interaction in explicit_pairwise_interactions:\n",
    "        interaction_split = interaction.split(\"/\")\n",
    "        dataset[f\"{interaction_split[0]}_{interaction_split[1]}\"] = dataset[interaction_split[0]] * dataset[interaction_split[1]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def reduce_features(dataset, config, target_variable):\n",
    "    if config[\"feature_reduction_method\"] == \"Tree-based\":\n",
    "        num_of_features_to_keep = int(config[\"num_of_features_to_keep\"])\n",
    "        num_of_trees = int(config[\"num_of_trees\"])\n",
    "        depth_of_trees = int(config[\"depth_of_trees\"])\n",
    "        \n",
    "        # Select features and target variable\n",
    "        X = dataset.drop(columns=[target_variable])\n",
    "        y = dataset[target_variable]\n",
    "        \n",
    "        # Initialize Random Forest Regressor\n",
    "        rf = RandomForestRegressor(n_estimators=num_of_trees, max_depth=depth_of_trees, random_state=42)\n",
    "        \n",
    "        # Fit Random Forest model\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "        \n",
    "        # Select top k features\n",
    "        top_features = feature_importances.nlargest(num_of_features_to_keep).index.tolist()\n",
    "        \n",
    "        # Update dataset with selected features\n",
    "        dataset = dataset[top_features + [target_variable]]\n",
    "        \n",
    "        return dataset\n",
    "    else:\n",
    "        print(\"Unsupported feature reduction method. Please choose 'Tree-based'.\")\n",
    "\n",
    "\n",
    "def partition_data(data, config):\n",
    "    \"\"\"\n",
    "    Partition data based on the configuration specified in the JSON.\n",
    "    \n",
    "    Args:\n",
    "    - data (DataFrame): The dataset to be partitioned.\n",
    "    - config (dict): JSON configuration specifying the partitioning details.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the partitioned data.\n",
    "    \"\"\"\n",
    "    # Extract configuration parameters\n",
    "    policy = config.get(\"policy\", \"Split the dataset\")\n",
    "    time_variable = config.get(\"time_variable\", None)\n",
    "    sampling_method = config.get(\"sampling_method\", \"No sampling(whole data)\")\n",
    "    split = config.get(\"split\", \"Randomly\")\n",
    "    k_fold = config.get(\"k_fold\", False)\n",
    "    train_ratio = config.get(\"train_ratio\", 0)\n",
    "    random_seed = config.get(\"random_seed\", None)\n",
    "    \n",
    "    # Partition data based on policy\n",
    "    if policy == \"Split the dataset\":\n",
    "        if sampling_method == \"No sampling(whole data)\":\n",
    "            if split == \"Randomly\":\n",
    "                if not k_fold:\n",
    "                    if train_ratio > 0:\n",
    "                        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=random_seed)\n",
    "                    else:\n",
    "                        raise ValueError(\"train_ratio must be greater than 0.\")\n",
    "                else:\n",
    "                    pass  \n",
    "            else:\n",
    "                raise ValueError(\"Only random split is implemented currently.\")\n",
    "        else:\n",
    "            raise ValueError(\"Sampling method other than 'No sampling(whole data)' is not implemented.\")\n",
    "    else:\n",
    "        raise ValueError(\"Policy other than 'Split the dataset' is not implemented.\")\n",
    "    \n",
    "    if k_fold:\n",
    "        kf = KFold(n_splits=k_fold, shuffle=True, random_state=random_seed)\n",
    "        fold_data = {}\n",
    "        fold_index = 1\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            fold_train_data, fold_test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            fold_data[f\"fold_{fold_index}\"] = {\"train\": fold_train_data, \"test\": fold_test_data}\n",
    "            fold_index += 1\n",
    "        return fold_data\n",
    "    else:\n",
    "        return {\"train\": pd.DataFrame(train_data), \"test\": pd.DataFrame(test_data)}\n",
    "\n",
    "\n",
    "\n",
    "def create_models_from_json(data):\n",
    "    models = {}\n",
    "    for key, value in data.items():\n",
    "        if value[\"is_selected\"] == True:\n",
    "            if key == \"RandomForestClassifier\":\n",
    "                model = RandomForestClassifier(n_estimators=value[\"max_trees\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf_min_value\"],\n",
    "                                                max_depth=value[\"max_depth\"])\n",
    "            elif key == \"RandomForestRegressor\":\n",
    "                model = RandomForestRegressor(n_estimators=value[\"max_trees\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf_min_value\"],\n",
    "                                                max_depth=value[\"max_depth\"])\n",
    "            elif key == \"GBTClassifier\":\n",
    "                model = GradientBoostingClassifier(n_estimators=value[\"fixed_number\"],\n",
    "                                                    max_depth=value[\"max_depth\"])\n",
    "            elif key == \"GBTRegressor\":\n",
    "                model = GradientBoostingRegressor(n_estimators=value[\"fixed_number\"],\n",
    "                                                    max_depth=value[\"max_depth\"])\n",
    "            elif key == \"LinearRegression\":\n",
    "                model = LinearRegression()\n",
    "            elif key == \"LogisticRegression\":\n",
    "                model = LogisticRegression()\n",
    "            elif key == \"RidgeRegression\":\n",
    "                model = Ridge()\n",
    "            elif key == \"LassoRegression\":\n",
    "                model = Lasso()\n",
    "            elif key == \"ElasticNetRegression\":\n",
    "                model = ElasticNet()\n",
    "            elif key == \"DecisionTreeRegressor\":\n",
    "                model = DecisionTreeRegressor(max_depth=value[\"max_depth\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"DecisionTreeClassifier\":\n",
    "                model = DecisionTreeClassifier(max_depth=value[\"max_depth\"],\n",
    "                                                min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"SVM\":\n",
    "                model = SVC()\n",
    "            elif key == \"SVR\":\n",
    "                model = SVR()\n",
    "            elif key == \"SGD\":\n",
    "                model = SGDRegressor(alpha=value[\"alpha_value\"][0])\n",
    "            elif key == \"KNN\":\n",
    "                model = KNeighborsClassifier(n_neighbors=value[\"k_value\"][0])\n",
    "            elif key == \"KNNRegressor\":\n",
    "                model = KNeighborsRegressor(n_neighbors=value[\"k_value\"][0])\n",
    "            elif key == \"extra_random_trees\":\n",
    "                model = ExtraTreesRegressor(n_estimators=value[\"num_of_trees\"][1],\n",
    "                                            max_depth=value[\"max_depth\"][1],\n",
    "                                            min_samples_leaf=value[\"min_samples_per_leaf\"][1])\n",
    "            elif key == \"neural_network\":\n",
    "                model = MLPClassifier(hidden_layer_sizes=value[\"hidden_layer_sizes\"])\n",
    "            elif key == \"neural_network_regressor\":\n",
    "                model = MLPRegressor(hidden_layer_sizes=value[\"hidden_layer_sizes\"])\n",
    "            elif key == \"xg_boost\":\n",
    "                model = xgb.XGBRegressor(n_estimators=value[\"max_num_of_trees\"],\n",
    "                                            max_depth=value[\"max_depth_of_tree\"][1],\n",
    "                                            learning_rate=value[\"learningRate\"][0],\n",
    "                                            reg_alpha=value[\"l1_regularization\"][0],\n",
    "                                            reg_lambda=value[\"l2_regularization\"][0],\n",
    "                                            gamma=value[\"gamma\"][0],\n",
    "                                            min_child_weight=value[\"min_child_weight\"][0],\n",
    "                                            subsample=value[\"sub_sample\"][0],\n",
    "                                            colsample_bytree=value[\"col_sample_by_tree\"][0],\n",
    "                                            early_stopping_rounds=value[\"early_stopping_rounds\"])\n",
    "\n",
    "            models[key] = model\n",
    "\n",
    "            return models\n",
    "def evaluate_models(models, metrics, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict on test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate each metric\n",
    "        model_results = {}\n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            model_results[metric_name] = metric_func(y_test, y_pred)\n",
    "        \n",
    "        # Store the results in the dictionary\n",
    "        results[model_name] = model_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_models_with_metrics(models, X_train, y_train, X_test, y_test, metrics):\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_prob = None\n",
    "\n",
    "        roc_auc = None\n",
    "        f1_optimal_threshold = None\n",
    "        lift_at_threshold = None\n",
    "\n",
    "        if y_prob is not None:\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        if metrics[\"optomize_model_hyperparameters_for\"] == \"AUC\" and roc_auc is not None:\n",
    "            model_metrics = roc_auc\n",
    "        elif metrics[\"optimize_threshold_for\"] == \"F1 Score\":\n",
    "            f1_optimal_threshold = 0\n",
    "            max_f1 = 0\n",
    "            for threshold in np.linspace(0.1, 0.9, 9):\n",
    "                if y_prob:\n",
    "                    y_pred_threshold = (y_prob >= threshold).astype(int)\n",
    "                    f1 = f1_score(y_test, y_pred_threshold)\n",
    "                    if f1 > max_f1:\n",
    "                        max_f1 = f1\n",
    "                        f1_optimal_threshold = threshold\n",
    "            model_metrics = max_f1\n",
    "        else:\n",
    "            model_metrics = None\n",
    "\n",
    "        if metrics[\"compute_lift_at\"] > 0 and y_prob is not None:\n",
    "            conf_matrix = confusion_matrix(y_test, y_prob >= f1_optimal_threshold)\n",
    "            true_positives = conf_matrix[1, 1]\n",
    "            lift_at_threshold = true_positives / (y_test.sum() / len(y_test))\n",
    "\n",
    "        results[name] = {\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"f1_optimal_threshold\": f1_optimal_threshold,\n",
    "            \"lift_at_threshold\": lift_at_threshold\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def apply_weighting_strategy(dataset, config):\n",
    "    if config[\"weighting_strategy_method\"] == \"Sample weights\":\n",
    "        weight_variable = config[\"weighting_strategy_weight_variable\"]\n",
    "        if weight_variable in dataset.columns:\n",
    "            sample_weights = dataset[weight_variable]\n",
    "            return sample_weights\n",
    "        else:\n",
    "            print(f\"Weight variable '{weight_variable}' not found in dataset. Weighting strategy not applied.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Unsupported weighting strategy method. Please choose 'Sample weights'.\")\n",
    "        return None\n",
    "\n",
    "def apply_probability_calibration(X_train, y_train, X_test, config):\n",
    "    if config[\"probability_calibration_method\"] == \"Sigmoid - Platt Scaling\":\n",
    "        calibrated_clf = CalibratedClassifierCV(method='sigmoid', cv='prefit')\n",
    "        calibrated_clf.fit(X_train, y_train)\n",
    "        calibrated_proba = calibrated_clf.predict_proba(X_test)\n",
    "        return calibrated_proba\n",
    "    else:\n",
    "        print(\"Unsupported probability calibration method. Please choose 'Sigmoid - Platt Scaling'.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def optimize_models(models, X_train, y_train, hyperparameters):\n",
    "    optimized_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hyperparameters[\"stratergy\"] == \"Grid Search\":\n",
    "            param_grid = {}  # Define hyperparameters grid here\n",
    "            if name == \"RandomForestClassifier\":\n",
    "                param_grid = {\n",
    "                    \"n_estimators\": [10, 20, 30],\n",
    "                    \"max_depth\": [None, 10, 20]\n",
    "                    # Add other hyperparameters specific to RandomForestClassifier\n",
    "                }\n",
    "            elif name == \"RandomForestRegressor\":\n",
    "                param_grid = {\n",
    "                    \"n_estimators\": [10, 20, 30],\n",
    "                    \"max_depth\": [None, 10, 20]\n",
    "                    # Add other hyperparameters specific to RandomForestRegressor\n",
    "                }\n",
    "            # Add similar elif blocks for other models\n",
    "\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=hyperparameters[\"num_of_folds\"],\n",
    "                                       scoring='roc_auc', n_jobs=hyperparameters[\"parallelism\"])\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            optimized_model = grid_search.best_estimator_\n",
    "            optimized_models[name] = optimized_model\n",
    "\n",
    "    return optimized_models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pwayk\\anaconda3\\envs\\jtm\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models {'RandomForestRegressor': RandomForestRegressor(min_samples_leaf=5, n_estimators=10)}\n",
      "metrics {'optomize_model_hyperparameters_for': 'AUC', 'optimize_threshold_for': 'F1 Score', 'compute_lift_at': 0, 'cost_matrix_gain_for_true_prediction_true_result': 1, 'cost_matrix_gain_for_true_prediction_false_result': 0, 'cost_matrix_gain_for_false_prediction_true_result': 0, 'cost_matrix_gain_for_false_prediction_false_result': 0}\n",
      "x_train      poly_petal_length_sepal_width_0  petal_width_sepal_length  \\\n",
      "137                              5.5                     11.52   \n",
      "84                               4.5                      8.10   \n",
      "27                               1.5                      1.04   \n",
      "127                              4.9                     10.98   \n",
      "132                              5.6                     14.08   \n",
      "..                               ...                       ...   \n",
      "9                                1.5                      0.49   \n",
      "103                              5.6                     11.34   \n",
      "67                               4.1                      5.80   \n",
      "117                              6.7                     16.94   \n",
      "47                               1.4                      0.92   \n",
      "\n",
      "     poly_petal_width_species_0  poly_petal_width_species_3  \n",
      "137                         1.8                         3.6  \n",
      "84                          1.5                         3.0  \n",
      "27                          0.2                         0.4  \n",
      "127                         1.8                         3.6  \n",
      "132                         2.2                         4.4  \n",
      "..                          ...                         ...  \n",
      "9                           0.1                         0.2  \n",
      "103                         1.8                         3.6  \n",
      "67                          1.0                         2.0  \n",
      "117                         2.2                         4.4  \n",
      "47                          0.2                         0.4  \n",
      "\n",
      "[120 rows x 4 columns]\n",
      "y_train 137    1.8\n",
      "84     1.5\n",
      "27     0.2\n",
      "127    1.8\n",
      "132    2.2\n",
      "      ... \n",
      "9      0.1\n",
      "103    1.8\n",
      "67     1.0\n",
      "117    2.2\n",
      "47     0.2\n",
      "Name: petal_width, Length: 120, dtype: float64\n",
      "X_test      poly_petal_length_sepal_width_0  petal_width_sepal_length  \\\n",
      "114                              5.1                     13.92   \n",
      "62                               4.0                      6.00   \n",
      "33                               1.4                      1.10   \n",
      "107                              6.3                     13.14   \n",
      "7                                1.5                      1.00   \n",
      "100                              6.0                     15.75   \n",
      "40                               1.3                      1.50   \n",
      "86                               4.7                     10.05   \n",
      "76                               4.8                      9.52   \n",
      "71                               4.0                      7.93   \n",
      "134                              5.6                      8.54   \n",
      "51                               4.5                      9.60   \n",
      "73                               4.7                      7.32   \n",
      "54                               4.6                      9.75   \n",
      "63                               4.7                      8.54   \n",
      "37                               1.5                      0.49   \n",
      "78                               4.5                      9.00   \n",
      "90                               4.4                      6.60   \n",
      "45                               1.4                      1.44   \n",
      "16                               1.3                      2.16   \n",
      "121                              4.9                     11.20   \n",
      "66                               4.5                      8.40   \n",
      "24                               1.9                      0.96   \n",
      "8                                1.4                      0.88   \n",
      "126                              4.8                     11.16   \n",
      "22                               1.0                      0.92   \n",
      "44                               1.9                      2.04   \n",
      "97                               4.3                      8.06   \n",
      "93                               3.3                      5.00   \n",
      "26                               1.6                      2.00   \n",
      "\n",
      "     poly_petal_width_species_0  poly_petal_width_species_3  \n",
      "114                         2.4                         4.8  \n",
      "62                          1.0                         2.0  \n",
      "33                          0.2                         0.4  \n",
      "107                         1.8                         3.6  \n",
      "7                           0.2                         0.4  \n",
      "100                         2.5                         5.0  \n",
      "40                          0.3                         0.6  \n",
      "86                          1.5                         3.0  \n",
      "76                          1.4                         2.8  \n",
      "71                          1.3                         2.6  \n",
      "134                         1.4                         2.8  \n",
      "51                          1.5                         3.0  \n",
      "73                          1.2                         2.4  \n",
      "54                          1.5                         3.0  \n",
      "63                          1.4                         2.8  \n",
      "37                          0.1                         0.2  \n",
      "78                          1.5                         3.0  \n",
      "90                          1.2                         2.4  \n",
      "45                          0.3                         0.6  \n",
      "16                          0.4                         0.8  \n",
      "121                         2.0                         4.0  \n",
      "66                          1.5                         3.0  \n",
      "24                          0.2                         0.4  \n",
      "8                           0.2                         0.4  \n",
      "126                         1.8                         3.6  \n",
      "22                          0.2                         0.4  \n",
      "44                          0.4                         0.8  \n",
      "97                          1.3                         2.6  \n",
      "93                          1.0                         2.0  \n",
      "26                          0.4                         0.8  \n",
      "y_test 137    1.8\n",
      "84     1.5\n",
      "27     0.2\n",
      "127    1.8\n",
      "132    2.2\n",
      "      ... \n",
      "9      0.1\n",
      "103    1.8\n",
      "67     1.0\n",
      "117    2.2\n",
      "47     0.2\n",
      "Name: petal_width, Length: 120, dtype: float64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m models \u001b[38;5;241m=\u001b[39m create_models_from_json(model_config)\n\u001b[0;32m     34\u001b[0m optimized_models \u001b[38;5;241m=\u001b[39m optimize_models(models, x_train, y_train, hyperparameters)\n\u001b[1;32m---> 36\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimized_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m results\n",
      "Cell \u001b[1;32mIn[27], line 238\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(models, metrics, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m    236\u001b[0m model_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name, metric_func \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 238\u001b[0m     model_results[metric_name] \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Store the results in the dictionary\u001b[39;00m\n\u001b[0;32m    241\u001b[0m results[model_name] \u001b[38;5;241m=\u001b[39m model_results\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_path = input_data['design_state_data']['session_info']['dataset']\n",
    "if os.path.exists(dataset_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "else:\n",
    "    raise FileExistsError(\"The Specified CSV not found\")\n",
    "\n",
    "all_features_to_handle = input_data['design_state_data']['feature_handling']\n",
    "target_column = input_data['design_state_data']['target']['target']\n",
    "features_to_generate = input_data['design_state_data']['feature_generation']\n",
    "feature_reduction_json  = input_data['design_state_data']['feature_reduction']\n",
    "model_config = input_data['design_state_data'][\"algorithms\"]\n",
    "training_config  = input_data['design_state_data']['train']\n",
    "metrics = input_data['design_state_data'][\"metrics\"]\n",
    "hyperparameters = input_data['design_state_data'][\"hyperparameters\"]\n",
    "\n",
    "X = data\n",
    "y = data[target_column]\n",
    "\n",
    "X = iterate_features_and_handle(all_features_to_handle, X)\n",
    "\n",
    "X = generate_features(X,features_to_generate )\n",
    "\n",
    "X = reduce_features(X, feature_reduction_json, target_column)\n",
    "\n",
    "X = partition_data(X, training_config)\n",
    "\n",
    "x_train = X['train'].drop(columns=[target_column])\n",
    "y_train = X['train'][target_column]\n",
    "x_test = X['test'].drop(columns=[target_column])\n",
    "y_test = X['train'][target_column]\n",
    "\n",
    "models = create_models_from_json(model_config)\n",
    "optimized_models = optimize_models(models, x_train, y_train, hyperparameters)\n",
    "\n",
    "\n",
    "# Skipping evaluation since all classification metrics are given and this is regression problem\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
